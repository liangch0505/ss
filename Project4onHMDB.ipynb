{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4\n",
    "Chen Liang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target of this project is to detect 'punching wall or something'. Dataset used is HMDB. Since the output will be a probability of 'punching' or not, the model is not designed to detect all categories in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Files from Json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath='hmdb/'\n",
    "punch_fnames=[join(fpath+'punch/', f) for f in listdir(fpath+'punch/') if isfile(join(fpath+'punch/', f))]\n",
    "#notpunch_fnames=[join(fpath+'notpunch/', f) for f in listdir(fpath+'notpunch/') if isfile(join(fpath+'notpunch/', f))]\n",
    "notpunch_fnames=[join(fpath+'mess/', f) for f in listdir(fpath+'mess/') if isfile(join(fpath+'mess/', f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Images from Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from video_to_img_hmdb import video_to_img\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Punch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 12\n",
    "_=Parallel(n_jobs=num_processes)(delayed(video_to_img)\\\n",
    "                               (str(video_id),'hframes/punch/',is_merged=True,merge_size=5) for video_id in punch_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not Punch\n",
    "Randomly choose same amount of video from dfnothit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = 12\n",
    "_=Parallel(n_jobs=num_processes)(delayed(video_to_img)\\\n",
    "                               (str(video_id),'hframes/notpunch/',is_merged=True,merge_size=5)\\\n",
    "                                 for video_id in notpunch_fnames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Organize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Note:</b> due to limited memory on laptop, only part of data is used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_path='hframes/'\n",
    "punch_imgs=[join(frame_path+'punch/', f) for f in listdir(frame_path+'punch/') if isfile(join(frame_path+'punch/', f))]\n",
    "notpunch_imgs=[join(frame_path+'notpunch/', f) for f in listdir(frame_path+'notpunch/') if isfile(join(frame_path+'notpunch/', f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "punch imgs:  8921 . not_punch imgs:  9446\n"
     ]
    }
   ],
   "source": [
    "print('punch imgs: ',len(punch_imgs),'. not_punch imgs: ',len(notpunch_imgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.randrange(2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w=224\n",
    "img_h=224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtl=[]\n",
    "for f in punch_imgs[500:]:\n",
    "    Xtl.append(cv2.imread(f))\n",
    "for f in notpunch_imgs[500:]:\n",
    "    Xtl.append(cv2.imread(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt=([1]*(len(punch_imgs)-500))+([0]*(len(notpunch_imgs)-500))\n",
    "yt=np.array(yt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtl, yt= shuffle(Xtl, yt)\n",
    "Xt=np.stack(Xtl,axis=0)\n",
    "del Xtl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xt=np.empty([8921+9446-1000,img_h,img_w,3],dtype='uint8')\n",
    "# i=0\n",
    "# p_i=500\n",
    "# np_i=500\n",
    "# for f in punch_imgs[500:]:\n",
    "#     Xt[i,:,:,:]=cv2.imread(f)\n",
    "#     i+=1\n",
    "# for f in notpunch_imgs[500:]:\n",
    "#     Xt[i,:,:,:]=cv2.imread(f)\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17367, 224, 224, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvl=[]\n",
    "xi=[]\n",
    "for f in punch_imgs[:500]:\n",
    "    Xvl.append(cv2.imread(f))\n",
    "    xi.append(1)\n",
    "for f in notpunch_imgs[:500]:\n",
    "    Xvl.append(cv2.imread(f))\n",
    "    xi.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "yv=[1]*500+[0]*500\n",
    "yv=np.array(yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xvl, yv= shuffle(Xvl, yv)\n",
    "Xv=np.stack(Xvl,axis=0)\n",
    "del Xvl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('qwe',Xt[17066,:,:,:])\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import stats\n",
    "import sklearn.metrics as metrices\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape=(224, 224, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model1=models.Sequential()\n",
    "    model1.add(layers.Conv2D(64,kernel_size=(3,3),strides=(2,2),activation='relu',input_shape=input_shape))\n",
    "    model1.add(layers.MaxPooling2D((2, 2)))\n",
    "    model1.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model1.add(layers.MaxPooling2D((2, 2)))\n",
    "    model1.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model1.add(layers.MaxPooling2D((2, 2)))\n",
    "    model1.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model1.add(layers.MaxPooling2D((2, 2)))\n",
    "    model1.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "    model1.add(layers.MaxPooling2D((2, 2)))\n",
    "    model1.add(layers.Dropout(0.25))\n",
    "    model1.add(layers.Flatten())\n",
    "    model1.add(layers.Dense(128, activation='relu'))\n",
    "    model1.add(layers.Dense(2, activation='softmax'))\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "ytb = to_categorical(yt)\n",
    "yvb = to_categorical(yv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 17367 samples, validate on 1000 samples\n",
      "Epoch 1/30\n",
      "17367/17367 [==============================] - 17s 972us/sample - loss: 0.3412 - accuracy: 0.8742 - val_loss: 0.4889 - val_accuracy: 0.9080\n",
      "Epoch 2/30\n",
      "17367/17367 [==============================] - 13s 756us/sample - loss: 0.0887 - accuracy: 0.9638 - val_loss: 0.2288 - val_accuracy: 0.9030\n",
      "Epoch 3/30\n",
      "17367/17367 [==============================] - 13s 761us/sample - loss: 0.0945 - accuracy: 0.9658 - val_loss: 0.6219 - val_accuracy: 0.9080\n",
      "Epoch 4/30\n",
      "17367/17367 [==============================] - 13s 761us/sample - loss: 0.0616 - accuracy: 0.9774 - val_loss: 0.6422 - val_accuracy: 0.9080\n",
      "Epoch 5/30\n",
      "17367/17367 [==============================] - 13s 759us/sample - loss: 0.0676 - accuracy: 0.9764 - val_loss: 1.3412 - val_accuracy: 0.9080\n",
      "Epoch 6/30\n",
      "17367/17367 [==============================] - 13s 760us/sample - loss: 0.0351 - accuracy: 0.9876 - val_loss: 0.9226 - val_accuracy: 0.9080\n",
      "Epoch 7/30\n",
      "17367/17367 [==============================] - 13s 758us/sample - loss: 0.0318 - accuracy: 0.9890 - val_loss: 1.7044 - val_accuracy: 0.8910\n",
      "Epoch 8/30\n",
      "17367/17367 [==============================] - 13s 756us/sample - loss: 0.0559 - accuracy: 0.9805 - val_loss: 1.2104 - val_accuracy: 0.7830\n",
      "Epoch 9/30\n",
      "17367/17367 [==============================] - 13s 761us/sample - loss: 0.0260 - accuracy: 0.9923 - val_loss: 0.6085 - val_accuracy: 0.9080\n",
      "Epoch 10/30\n",
      "17367/17367 [==============================] - 13s 762us/sample - loss: 0.0188 - accuracy: 0.9937 - val_loss: 3.2216 - val_accuracy: 0.8390\n",
      "Epoch 11/30\n",
      "17367/17367 [==============================] - 13s 762us/sample - loss: 0.0141 - accuracy: 0.9955 - val_loss: 2.0293 - val_accuracy: 0.9080\n",
      "Epoch 12/30\n",
      "17367/17367 [==============================] - 13s 765us/sample - loss: 0.0364 - accuracy: 0.9876 - val_loss: 3.1092 - val_accuracy: 0.9080\n",
      "Epoch 13/30\n",
      "17367/17367 [==============================] - 13s 763us/sample - loss: 0.0657 - accuracy: 0.9831 - val_loss: 1.9914 - val_accuracy: 0.8810\n",
      "Epoch 14/30\n",
      "17367/17367 [==============================] - 13s 763us/sample - loss: 0.0088 - accuracy: 0.9968 - val_loss: 3.2138 - val_accuracy: 0.9080\n",
      "Epoch 15/30\n",
      "17367/17367 [==============================] - 13s 771us/sample - loss: 0.0159 - accuracy: 0.9944 - val_loss: 1.6298 - val_accuracy: 0.9080\n",
      "Epoch 16/30\n",
      "17367/17367 [==============================] - 13s 764us/sample - loss: 0.0269 - accuracy: 0.9907 - val_loss: 2.3332 - val_accuracy: 0.9070\n",
      "Epoch 17/30\n",
      "17367/17367 [==============================] - 14s 784us/sample - loss: 0.0345 - accuracy: 0.9902 - val_loss: 2.5982 - val_accuracy: 0.8140\n",
      "Epoch 18/30\n",
      "17367/17367 [==============================] - 13s 751us/sample - loss: 0.0148 - accuracy: 0.9951 - val_loss: 4.6564 - val_accuracy: 0.8650\n",
      "Epoch 19/30\n",
      "17367/17367 [==============================] - 13s 742us/sample - loss: 0.0186 - accuracy: 0.9942 - val_loss: 6.3598 - val_accuracy: 0.9030\n",
      "Epoch 20/30\n",
      "17367/17367 [==============================] - 13s 747us/sample - loss: 0.0204 - accuracy: 0.9946 - val_loss: 3.0194 - val_accuracy: 0.7010\n",
      "Epoch 21/30\n",
      "17367/17367 [==============================] - 13s 746us/sample - loss: 0.0102 - accuracy: 0.9970 - val_loss: 7.3955 - val_accuracy: 0.9080\n",
      "Epoch 22/30\n",
      "17367/17367 [==============================] - 13s 744us/sample - loss: 0.0035 - accuracy: 0.9990 - val_loss: 4.8839 - val_accuracy: 0.9080\n",
      "Epoch 23/30\n",
      "17367/17367 [==============================] - 13s 743us/sample - loss: 8.6623e-04 - accuracy: 0.9996 - val_loss: 9.1893 - val_accuracy: 0.9080\n",
      "Epoch 24/30\n",
      "17367/17367 [==============================] - 13s 746us/sample - loss: 0.0308 - accuracy: 0.9916 - val_loss: 1.1467 - val_accuracy: 0.9020\n",
      "Epoch 25/30\n",
      "17367/17367 [==============================] - 13s 747us/sample - loss: 0.0098 - accuracy: 0.9965 - val_loss: 4.8716 - val_accuracy: 0.9080\n",
      "Epoch 26/30\n",
      "17367/17367 [==============================] - 13s 751us/sample - loss: 0.0345 - accuracy: 0.9921 - val_loss: 6.8915 - val_accuracy: 0.9080\n",
      "Epoch 27/30\n",
      "17367/17367 [==============================] - 13s 746us/sample - loss: 0.0265 - accuracy: 0.9931 - val_loss: 4.9537 - val_accuracy: 0.9080\n",
      "Epoch 28/30\n",
      "17367/17367 [==============================] - 14s 821us/sample - loss: 0.0100 - accuracy: 0.9971 - val_loss: 4.9233 - val_accuracy: 0.908099 - accuracy\n",
      "Epoch 29/30\n",
      "17367/17367 [==============================] - 14s 780us/sample - loss: 0.0237 - accuracy: 0.9953 - val_loss: 7.5748 - val_accuracy: 0.8680\n",
      "Epoch 30/30\n",
      "17367/17367 [==============================] - 13s 771us/sample - loss: 0.0044 - accuracy: 0.9984 - val_loss: 5.2514 - val_accuracy: 0.8680\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(Xt, yt, epochs=30, \n",
    "                    validation_data=(Xv, yv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\clle1\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: hmdbmodel2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('hmdbmodel2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
